\documentclass[9pt]{IEEEtran}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{booktabs}  % For better tables
\usepackage{adjustbox} % For table width adjustment

\title{\vspace{0ex}Performance Analysis of Cache Coherence Protocols and Interconnection Networks in Multiprocessor Systems}
\author{Erik Pahor, Jaka Å kerjanc\vspace{-4.0ex}}

\begin{document}

\maketitle

\section{Introduction}
This report presents an analysis of three key aspects of multiprocessor system performance: snooping-based cache coherence protocols, the impact of false sharing on directory-based protocols, and the performance of different interconnection network topologies. Using the GEM5 simulator, we evaluated these aspects across different processor configurations (2, 4, 8, and 16 cores) using various workloads including Cholesky decomposition, PI calculation, and the STREAM benchmark.

\section{Snooping-based Cache Coherence Protocol Analysis}
For the snooping-based protocol, we evaluated the performance of Cholesky decomposition across different processor counts and analyzed several key metrics.

\subsection{Results and Analysis}

\begin{table}[htbp]
\centering
\caption{Snooping-based Protocol Performance Metrics}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
Core Count & CPI & L1D Miss Ratio & Upgrade Requests & Snoop Traffic \\
\midrule
2  & 3.4359 & 0.0164 & 2,439 & 309,376 \\
4  & 3.2925 & 0.0204 & 4,372 & 854,080 \\
8  & 3.0917 & 0.0246 & 7,777 & 1,977,856 \\
16 & 2.9745 & 0.0212 & 14,576 & 4,403,904 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{CPI (Cycles Per Instruction):} We observe a consistent decrease in CPI as the number of processors increases, from 3.44 with 2 cores to 2.97 with 16 cores. This indicates better utilization of computational resources with higher core counts for this workload, suggesting that Cholesky decomposition parallelizes effectively.

\textbf{L1D Miss Ratio:} The L1 data cache miss ratio increases with the number of processors up to 8 cores (from 0.0164 to 0.0246), but then slightly decreases at 16 cores (to 0.0212). This non-linear behavior suggests that while more processors initially create more contention for cache lines, at 16 cores the workload distribution may allow for better cache locality in each core.

\textbf{Upgrade Requests:} The number of upgrade requests increases almost linearly with the number of processors, from 2,439 with 2 cores to 14,576 with 16 cores. This reflects the growing need for exclusive access to shared cache lines as more cores simultaneously process the workload.

\textbf{Snoop Traffic:} Snoop traffic increases substantially with processor count, growing from 309,376 with 2 cores to 4,403,904 with 16 cores. This represents a 14x increase for an 8x increase in core count, demonstrating the significant overhead of maintaining cache coherence as the system scales.

\section{Impact of False Sharing on Directory-based Protocols}
We analyzed the performance impact of false sharing by comparing two implementations of a PI calculation algorithm - one prone to false sharing and one optimized to avoid it.

\subsection{Results and Analysis}

\textbf{CPI Comparison:}

\begin{table}[htbp]
\centering
\caption{CPI Comparison Between False Sharing and Optimized Implementations}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & False Sharing CPI & Optimized CPI & Difference (\%) \\
\midrule
2  & 2.504 & 2.295 & 9.1\% \\
4  & 3.001 & 2.296 & 30.7\% \\
8  & 2.998 & 2.298 & 30.5\% \\
16 & 3.097 & 2.301 & 34.6\% \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

False sharing consistently results in higher CPI across all processor counts, with the gap widening as the number of cores increases. At 16 cores, the CPI with false sharing is nearly 35\% higher than the optimized version.

\textbf{Execution Time:}

\begin{table}[htbp]
\centering
\caption{Execution Time Comparison}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & False Sharing (cycles) & Optimized (cycles) & Slowdown Factor \\
\midrule
2  & 13,155,904,593 & 12,073,162,419 & 1.09x \\
4  & 8,035,984,638  & 6,205,135,986  & 1.30x \\
8  & 4,193,290,845  & 3,281,357,025  & 1.28x \\
16 & 2,623,817,889  & 1,834,961,535  & 1.43x \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

The execution time data shows that false sharing creates a significant performance penalty, which becomes more pronounced with higher core counts. At 16 cores, the false sharing version is 43\% slower than the optimized version.

\textbf{Invalidations:}

\begin{table}[htbp]
\centering
\caption{Cache Invalidations Comparison}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & False Sharing & Optimized & Ratio \\
\midrule
2  & 500,351 & 327 & 1,530x \\
4  & 750,395 & 390 & 1,924x \\
8  & 750,528 & 548 & 1,370x \\
16 & 672,897 & 814 & 827x \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

The number of invalidations shows the most dramatic difference between the two versions. The false sharing implementation generates orders of magnitude more invalidations than the optimized version, highlighting how false sharing forces unnecessary coherence traffic.

\textbf{Memory Access Patterns:}
\begin{itemize}
  \item False sharing implementation shows fewer loads from the M state (modified) compared to the optimized version.
  \item The optimized version shows more efficient cache utilization with fewer transfers between cache coherence states.
  \item L1\_GETS and L1\_GETX requests are significantly higher in the false sharing version.
\end{itemize}

\textbf{Network Traffic:}
\begin{itemize}
  \item Network request control messages are 3-4 orders of magnitude higher in the false sharing version.
  \item Response data traffic follows a similar pattern, showing the coherence protocol overhead.
  \item Writeback data traffic remains relatively constant across implementations, suggesting similar actual data modification rates.
\end{itemize}

\section{Interconnection Network Performance}
We evaluated three different network topologies (SimplePt2Pt, Circle/Ring, and Crossbar) using the STREAM benchmark across different processor counts.

\subsection{Results and Analysis}

\textbf{Network Traffic Comparison:}

\begin{table}[htbp]
\centering
\caption{Request Control Traffic}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & SimplePt2Pt & Circle & Crossbar \\
\midrule
2  & 7,948   & 11,931    & 11,484 \\
4  & 253,228 & 561,728   & 378,311 \\
8  & 405,694 & 1,247,391 & 587,823 \\
16 & 764,842 & 3,919,500 & 1,139,822 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\centering
\caption{Response Data Traffic}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & SimplePt2Pt & Circle & Crossbar \\
\midrule
2  & 4,026,888 & 6,484,447  & 6,040,032 \\
4  & 4,030,964 & 8,381,114  & 6,045,455 \\
8  & 4,041,748 & 12,190,192 & 6,060,095 \\
16 & 4,063,940 & 19,874,359 & 6,094,745 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\centering
\caption{Writeback Data Traffic}
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
Core Count & SimplePt2Pt & Circle & Crossbar \\
\midrule
2  & 1,001,706 & 1,440,128 & 1,502,559 \\
4  & 1,001,484 & 2,316,615 & 1,502,226 \\
8  & 1,001,596 & 4,070,546 & 1,502,259 \\
16 & 1,004,484 & 7,593,664 & 1,507,335 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Analysis by Network Topology:}

\begin{enumerate}
  \item \textbf{SimplePt2Pt Network:}
  \begin{itemize}
    \item Shows the lowest overall traffic across all core counts
    \item Request control traffic increases linearly with core count
    \item Response data traffic remains relatively stable regardless of core count
    \item Writeback data traffic is essentially constant across different core counts
  \end{itemize}

  \item \textbf{Circle (Ring) Network:}
  \begin{itemize}
    \item Generates the highest traffic volumes across all metrics
    \item Shows super-linear growth in all traffic types as core count increases
    \item At 16 cores, request control traffic is 5.1x higher than SimplePt2Pt and 3.4x higher than Crossbar
    \item Response data traffic at 16 cores is 4.9x higher than SimplePt2Pt
    \item The inefficiency becomes more pronounced with scale
  \end{itemize}

  \item \textbf{Crossbar Network:}
  \begin{itemize}
    \item Shows moderate traffic growth with scaling
    \item Request control traffic increases at a rate between SimplePt2Pt and Circle
    \item Response data and writeback traffic remain relatively stable with increasing core count
    \item Provides a middle ground between the simplicity of point-to-point and the high traffic of the ring topology
  \end{itemize}
\end{enumerate}

\section{Conclusion}
Our analysis reveals several key insights about multiprocessor system design:

\begin{enumerate}
  \item \textbf{Snooping-based Cache Coherence:}
  \begin{itemize}
    \item Provides good performance scaling for parallel workloads like Cholesky decomposition
    \item However, snoop traffic increases dramatically with core count, potentially limiting scalability beyond 16 cores
    \item The protocol can maintain reasonable miss ratios even at higher core counts
  \end{itemize}

  \item \textbf{False Sharing Impact:}
  \begin{itemize}
    \item False sharing severely degrades performance in directory-based protocols
    \item The performance penalty increases with core count due to exponentially more coherence traffic
    \item Optimizing code to avoid false sharing can yield performance improvements of 30-40\%
    \item The enormous difference in invalidation counts (up to 1900x) highlights the fundamental mechanism of the performance degradation
  \end{itemize}

  \item \textbf{Interconnection Networks:}
  \begin{itemize}
    \item Network topology plays a crucial role in system scalability
    \item SimplePt2Pt networks show the best traffic characteristics but may have practical implementation limitations
    \item Ring topologies suffer from severe congestion at higher core counts
    \item Crossbar networks provide a reasonable compromise, showing moderate traffic growth with scaling
  \end{itemize}
\end{enumerate}

These findings emphasize the importance of considering both software optimizations (avoiding false sharing) and hardware design choices (coherence protocols and network topologies) when building scalable multiprocessor systems. The optimal configuration depends on the specific workload characteristics, with different design choices showing distinct advantages for different application patterns.

\end{document}
